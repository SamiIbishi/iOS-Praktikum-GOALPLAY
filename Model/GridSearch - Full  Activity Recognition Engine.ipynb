{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (1) Import all relevant libraries, layer and classes <h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Python libraries\n",
    "import numpy as np, collections\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "import keras.backend as k\n",
    "from keras import optimizers\n",
    "from keras import losses, regularizers\n",
    "from keras import initializers\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve, GridSearchCV\n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sn\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Locale methods and properties\n",
    "from auxilary_functions import time_stamp\n",
    "from load_data import load_and_store_data, load_stored_data \n",
    "from model import create_compiled_model\n",
    "from callbacks import get_callbacks \n",
    "from plot_pose_sequence import plot_trainings_data, plot_test_data\n",
    "from plot_confusion_matrix import print_confusion_matrix \n",
    "from plot_trainings_history import plot_loss_acc\n",
    "from plot_data_distribution import plot_data_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (2) Load and prepare goalkeeper pose data <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./stored_data/\"\n",
    "\n",
    "# Load data from .npy files \n",
    "# (Framrates/Poses-per-sequence: '5' / '10' / '15' / '30' / '45' / '60' / '75' / '90')\n",
    "X, y = load_stored_data(dir_path=DATA_PATH, num_poses='90')\n",
    "# X = [samples, timesteps, features]\n",
    "# y = [samples, labels]\n",
    "\n",
    "# Split dataset into train and test set (and shuffle them)\n",
    "X_train, X_test, y_train_temp, y_test_temp = train_test_split(X, y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "# Get one hot vector from labels\n",
    "y_train = to_categorical(y_train_temp, num_classes=6)\n",
    "y_test = to_categorical(y_test_temp, num_classes=6)\n",
    "\n",
    "# Input features\n",
    "INPUT = [\n",
    "    \"HEAD_X\",\n",
    "    \"HEAD_Y_\",\n",
    "    \"BODY_X\",\n",
    "    \"BODY_Y\",\n",
    "    \"LEFT_ARM_X\",\n",
    "    \"LEFT_ARM_Y\",\n",
    "    \"RIGHT_ARM_X\",\n",
    "    \"RIGHT_ARM_Y\",\n",
    "    \"LEFT_LEG_X\",\n",
    "    \"LEFT_LEG_Y\",\n",
    "    \"RIGHT_LEG_X\",\n",
    "    \"RIGHT_LEG_Y\",\n",
    "]\n",
    "\n",
    "# Output classes\n",
    "LABELS = [    \n",
    "    \"STAND_BY\",\n",
    "    \"LEFT_DIVE\",\n",
    "    \"LONG_LEFT_DIVE\",\n",
    "    \"RIGHT_DIVE\",\n",
    "    \"LONG_RIGHT_DIVE\",\n",
    "    \"HIGH_CATCH\",\n",
    "]\n",
    "\n",
    "# Some debugging info\n",
    "print(\"---------------------------------------\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Bsic information regardin the data:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Trainings data...\")\n",
    "print(\"... input shape=\" + str(X_train.shape))\n",
    "print(\"... target shape=\" + str(y_train.shape))\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Test data...\")\n",
    "print(\"... input shape=\" + str(X_test.shape))\n",
    "print(\"... target shape=\" + str(y_test.shape))\n",
    "print(\"---------------------------------------\")\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data distribution (trainings data)\n",
    "plot_data_distribution(y_train_temp, labels=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data distribution (teest data)\n",
    "plot_data_distribution(y_test_temp, labels=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (3) Define constants and additional parameters <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input Data\n",
    "n_timesteps = X_train.shape[1] #len(X_train[1])  # n-timesteps per series per series\n",
    "n_features = X_train.shape[2] #len(X_train[0][0])  # n input parameters per timestep\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "input_shape = (n_timesteps, n_features)\n",
    "n_mem_units = 180 # Hidden layer num of features\n",
    "n_classes = len(LABELS) # n classes (should go up, or should go down)\n",
    "\n",
    "# Training - Hyperparameter  \n",
    "learning_rate = 0.0008\n",
    "optimizers = [optimizers.RMSprop(lr=learning_rate, decay=0.5), \n",
    "              optimizers.Adam(lr=learning_rate, decay=0.5),\n",
    "              optimizers.Nadam(lr=learning_rate)]\n",
    "num_epochs = 42\n",
    "batch_size = [256, 512]\n",
    "dropout = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Parameter Grid \n",
    "param_grid = { \n",
    "    'optimizer':optimizers, \n",
    "    'batch_size':batch_size,\n",
    "    'dropout':dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (4) Define and build sequence model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Weight classes to overcome the unbalanced data\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_temp), y_train_temp)\n",
    "\n",
    "# Create model\n",
    "model = KerasClassifier(build_fn=create_compiled_model, input_shape=input_shape, class_weights)\n",
    "\n",
    "# Generate Grid Search Model with above parameters\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, verbose=0, refit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (6) Train model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape model fo CoreML model \n",
    "reshaped_X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "print(reshaped_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model on trainings data \n",
    "grid_result = grid_search.fit(reshaped_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train score - \" + str(grid_search.score(reshaped_X_train, y_train)))\n",
    "\n",
    "reshaped_X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "print(\"test score - \" + str(grid_search.score(reshaped_X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (7) Predict and evaluate <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_result.sort_values('rank_test_score')\n",
    "cv_result.to_csv(r'results_90p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_best_param = pd.DataFrame(grid_search.best_params_, index=[0])\n",
    "cv_best_param.to_csv(r'best_param_90p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('best_param_90p.csv') \n",
    "var = df['dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = df.to_dict('records')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params[0]['dropout'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
