{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (1) Import all relevant libraries, layer and classes <h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0708 11:33:25.517977 140242256430912 __init__.py:118] TensorFlow version 1.14.0 detected. Last version known to be fully compatible is 1.12.0 .\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Python libraries\n",
    "import numpy as np\n",
    "import pydot as pyd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras import optimizers\n",
    "from keras import metrics, losses, regularizers\n",
    "from keras import initializers\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, TimeDistributed, Lambda, Flatten\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras.backend as K\n",
    "\n",
    "# Scikit-learn\n",
    "import sklearn.preprocessing as skprep\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CoreML \n",
    "import coremltools\n",
    "\n",
    "# Keypoint loader\n",
    "#from ipynb.fs.full.Keypoint_loader import *\n",
    "from auxilary_functions import *\n",
    "#from PlotTrainingsHistory import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (2) Load and prepare goalkeeper pose data <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Data/TrainingsVideo/Exercises/\"\n",
    "\n",
    "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "# load input/output trainings data\n",
    "# X_train = [samples, timesteps, features]\n",
    "# y_train = [samples, labels]\n",
    "#X_train = load_data(X_train_path)\n",
    "X_train, y_train = load_data()\n",
    "\n",
    "\n",
    "# load input/output test data\n",
    "#X_test = load_X(X_test_path)\n",
    "#y_test = load_y(y_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (3) Define constants and additional parameters <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bsic information regardin the data:\n",
      "---------------------------------------\n",
      "Trainings data...\n",
      "... input shape=(146, 84, 24)\n",
      "... input mean=262.1317881604697\n",
      "... input std=156.11027205208515\n",
      "... target shape=(146, 2)\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input features\n",
    "INPUT = [\n",
    "    \"HEAD_X\",\n",
    "    \"HEAD_Y_\",\n",
    "    \"BODY_X\",\n",
    "    \"BODY_Y\",\n",
    "    \"LEFT_ARM_X\",\n",
    "    \"LEFT_ARM_Y\",\n",
    "    \"RIGHT_ARM_X\",\n",
    "    \"RIGHT_ARM_Y\",\n",
    "    \"LEFT_LEG_X\",\n",
    "    \"LEFT_LEG_Y\",\n",
    "    \"RIGHT_LEG_X\",\n",
    "    \"RIGHT_LEG_Y\",\n",
    "]\n",
    "\n",
    "# Output classes\n",
    "LABELS = [    \n",
    "    #\"SHORT_LEFT_DIVE\",\n",
    "    \"LEFT_DIVE\",\n",
    "    #\"LONG_LEFT_DIVE\",\n",
    "    #\"SHORT_RIGHT_DIVE\",\n",
    "    \"RIGHT_DIVE\",\n",
    "    #\"LONG_RIGHT_DIVE\",\n",
    "    #\"LOW_CATCH\",\n",
    "    #\"MIDDLE_CATCH\",\n",
    "    #\"HIGH_CATCH\",\n",
    "]\n",
    "\n",
    "# Input Data\n",
    "n_timesteps = 84 #len(X_train[1])  # n-timesteps per series per series\n",
    "n_features = 24 #len(X_train[0][0])  # n input parameters per timestep\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "input_shape = (n_timesteps, n_features)\n",
    "n_mem_units = 42 # Hidden layer num of features\n",
    "n_classes = LABELS.count # n classes (should go up, or should go down)\n",
    "\n",
    "# Training - Hyperparameter  \n",
    "learning_rate = 0.002 \n",
    "optimizer = optimizers.Adam(lr=learning_rate, decay=0.2) \n",
    "n_epochs = 20  \n",
    "batch_size = 16\n",
    "\n",
    "# Some debugging info\n",
    "print(\"Bsic information regardin the data:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Trainings data...\")\n",
    "print(\"... input shape=\" + str(X_train.shape))\n",
    "print(\"... input mean=\" + str(np.mean(X_train)))\n",
    "print(\"... input std=\" + str(np.std(X_train)))\n",
    "print(\"... target shape=\" + str(y_train.shape))\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (4) Define and build sequence model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 11:33:26.332987 140242256430912 deprecation_wrapper.py:119] From /home/sami/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0708 11:33:26.337445 140242256430912 deprecation_wrapper.py:119] From /home/sami/anaconda3/envs/tf/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start sequentially defining model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(lambda x: x ** 2))\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "## Output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "# Build model \n",
    "model.compile(loss=losses.binary_crossentropy, \n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae', 'accuracy'])  # for binary classfication\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', \n",
    "#              optimizer=optimizer,\n",
    "#              metrics=['mae', 'accuracy'])  # if more than two targets\n",
    "\n",
    "# Model abstract information\n",
    "model.author = 'Goalplay'\n",
    "model.short_description = 'Activity Recognition with goalplayer training'\n",
    "\n",
    "# Visualize model\n",
    "plot_model(model, to_file='./training_history/model_visualize/model_' + time_stamp() + '.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (5) Record and store process of models/training <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "logdir = './test_lambda/logs/' + time_stamp()\n",
    "\n",
    "## Check whether the target directoy exist and \n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "    \n",
    "## Create callback for tensorboard/trainings-history with the path to the logs directory\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "# Generate callback list \n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (6) Train model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 11:33:26.626701 140242256430912 deprecation_wrapper.py:119] From /home/sami/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0708 11:33:26.651787 140242256430912 deprecation_wrapper.py:119] From /home/sami/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0708 11:33:26.678493 140242256430912 deprecation_wrapper.py:119] From /home/sami/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0708 11:33:26.684466 140242256430912 deprecation.py:323] From /home/sami/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0708 11:33:26.859915 140242256430912 deprecation_wrapper.py:119] From /home/sami/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102 samples, validate on 44 samples\n",
      "Epoch 1/20\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 2/20\n",
      "102/102 [==============================] - 0s 141us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 3/20\n",
      "102/102 [==============================] - 0s 246us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 4/20\n",
      "102/102 [==============================] - 0s 165us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 5/20\n",
      "102/102 [==============================] - 0s 268us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 6/20\n",
      "102/102 [==============================] - 0s 332us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 7/20\n",
      "102/102 [==============================] - 0s 166us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 8/20\n",
      "102/102 [==============================] - 0s 172us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 9/20\n",
      "102/102 [==============================] - 0s 222us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 10/20\n",
      "102/102 [==============================] - 0s 153us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 11/20\n",
      "102/102 [==============================] - 0s 182us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 12/20\n",
      "102/102 [==============================] - 0s 156us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 13/20\n",
      "102/102 [==============================] - 0s 259us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 14/20\n",
      "102/102 [==============================] - 0s 162us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 15/20\n",
      "102/102 [==============================] - 0s 413us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 16/20\n",
      "102/102 [==============================] - 0s 175us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 17/20\n",
      "102/102 [==============================] - 0s 225us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 18/20\n",
      "102/102 [==============================] - 0s 163us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 19/20\n",
      "102/102 [==============================] - 0s 163us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n",
      "Epoch 20/20\n",
      "102/102 [==============================] - 0s 181us/step - loss: 11.4726 - mean_absolute_error: 0.7157 - acc: 0.2843 - val_loss: 15.3016 - val_mean_absolute_error: 0.9545 - val_acc: 0.0455\n"
     ]
    }
   ],
   "source": [
    "# Train model on trainings data \n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    verbose=1, \n",
    "                    shuffle=True, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_split=0.3, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (8) Save model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = './test_lambda/model_2019-07-08-11-33-49.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-355df0162b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Convert trained model into CoreML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcoreml_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoremltools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./test_lambda/model_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime_stamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Store CoreML model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/coremltools/converters/keras/_keras_converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model, input_names, output_names, image_input_names, input_name_shape_dict, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, model_precision, predicted_probabilities_output, add_custom_layers, custom_conversion_functions)\u001b[0m\n\u001b[1;32m    758\u001b[0m                       \u001b[0mpredicted_probabilities_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                       \u001b[0madd_custom_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                       custom_conversion_functions=custom_conversion_functions)\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_MLModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/coremltools/converters/keras/_keras_converter.py\u001b[0m in \u001b[0;36mconvertToSpec\u001b[0;34m(model, input_names, output_names, image_input_names, input_name_shape_dict, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, model_precision, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, custom_objects)\u001b[0m\n\u001b[1;32m    554\u001b[0m                                            \u001b[0madd_custom_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_custom_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                                            \u001b[0mcustom_conversion_functions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_conversion_functions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                                            custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    557\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         raise RuntimeError(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/coremltools/converters/keras/_keras2_converter.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(model, input_names, output_names, image_input_names, input_name_shape_dict, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, custom_objects)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_string_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = './test_lambda/model_2019-07-08-11-33-49.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "model.save(\"./test_lambda/model_\" + time_stamp() + \".h5\")\n",
    "\n",
    "# Convert trained model into CoreML\n",
    "coreml_model = coremltools.converters.keras.convert(\"./test_lambda/model_\" + time_stamp() + \".h5\")\n",
    "\n",
    "# Store CoreML model \n",
    "coreml_model.save(\"./test_lambda/mlmodel_\" + time_stamp() + \".mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
