{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (1) Import all relevant libraries, layer and classes <h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 11:15:38.114793 139746819770176 __init__.py:71] TensorFlow version 1.14.0 detected. Last version known to be fully compatible is 1.13.1 .\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a875521302ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Keypoint loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauxilary_functions\u001b[0m \u001b[0;31m#, keypoint_reader, poses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Dataloader'"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Python libraries\n",
    "import numpy as np, collections\n",
    "import pydot as pyd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras import optimizers\n",
    "from keras import metrics, losses, regularizers\n",
    "from keras import initializers\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, TimeDistributed\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Scikit-learn\n",
    "import sklearn.preprocessing as skprep\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# CoreML \n",
    "import coremltools\n",
    "import tfcoreml as tf_converter\n",
    "\n",
    "# Keypoint loader\n",
    "import auxilary_functions #, keypoint_reader, poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (2) Load and prepare goalkeeper pose data <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Data/TrainingsVideo/Exercises/\"\n",
    "\n",
    "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "# load input/output trainings data\n",
    "# X_train = [samples, timesteps, features]\n",
    "# y_train = [samples, labels]\n",
    "X_train, y_train, y = load_all_data()\n",
    "\n",
    "#printPoseSequence(X_train[150][62][:])\n",
    "\n",
    "# load input/output test data\n",
    "#X_test = load_X(X_test_path)\n",
    "#y_test = load_y(y_test_path)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (3) Define constants and additional parameters <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input features\n",
    "INPUT = [\n",
    "    \"HEAD_X\",\n",
    "    \"HEAD_Y_\",\n",
    "    \"BODY_X\",\n",
    "    \"BODY_Y\",\n",
    "    \"LEFT_ARM_X\",\n",
    "    \"LEFT_ARM_Y\",\n",
    "    \"RIGHT_ARM_X\",\n",
    "    \"RIGHT_ARM_Y\",\n",
    "    \"LEFT_LEG_X\",\n",
    "    \"LEFT_LEG_Y\",\n",
    "    \"RIGHT_LEG_X\",\n",
    "    \"RIGHT_LEG_Y\",\n",
    "]\n",
    "\n",
    "# Output classes\n",
    "LABELS = [    \n",
    "    \"SHORT_LEFT_DIVE\",\n",
    "    \"LEFT_DIVE\",\n",
    "    \"LONG_LEFT_DIVE\",\n",
    "    \"SHORT_RIGHT_DIVE\",\n",
    "    \"RIGHT_DIVE\",\n",
    "    \"LONG_RIGHT_DIVE\",\n",
    "    #\"LOW_CATCH\",\n",
    "    #\"MIDDLE_CATCH\",\n",
    "    #\"HIGH_CATCH\",\n",
    "]\n",
    "\n",
    "# Input Data\n",
    "n_timesteps = 139 #len(X_train[1])  # n-timesteps per series per series\n",
    "n_features = 24 #len(X_train[0][0])  # n input parameters per timestep\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "input_shape = (n_timesteps, n_features)\n",
    "n_mem_units = 69 # Hidden layer num of features\n",
    "n_classes = len(LABELS) # n classes (should go up, or should go down)\n",
    "\n",
    "# Training - Hyperparameter  \n",
    "learning_rate = 0.004 \n",
    "optimizer = optimizers.RMSprop(lr=learning_rate, decay=256e-4) \n",
    "n_epochs = 10  \n",
    "batch_size = 48\n",
    "\n",
    "# Some debugging info\n",
    "print(\"Bsic information regardin the data:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Trainings data...\")\n",
    "print(\"... input shape=\" + str(X_train.shape))\n",
    "print(\"... input mean=\" + str(np.mean(X_train)))\n",
    "print(\"... input std=\" + str(np.std(X_train)))\n",
    "print(\"... target shape=\" + str(y_train.shape))\n",
    "print(\"---------------------------------------\")\n",
    "#print(\"Test data...\")\n",
    "#print(\"... input shape=\" + str(X_test.shape))\n",
    "#print(\"... input mean=\" + str(np.mean(X_test)))\n",
    "#print(\"... input std=\" + str(np.std(X_test)))\n",
    "#print(\"... target shape=\" + str(y_train.shape))\n",
    "#print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (4) Define and build sequence model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_model(input_shape, n_mem_units, n_classes, optimizer=optimizer, loss=losses.categorical_crossentropy, init='glorot_uniform')\n",
    "\n",
    "# Print model details/summary\n",
    "model.summary()\n",
    "\n",
    "# Build model \n",
    "model.compile(loss=losses.categorical_crossentropy, \n",
    "              optimizer=optimizer,\n",
    "              metrics=['mse', 'accuracy'])  # for binary classfication\n",
    "\n",
    "\n",
    "\n",
    "# Visualize model\n",
    "plot_model(model, to_file='./training_history/model_visualize/model_' + time_stamp() + '.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (5) Record and store process of models/training <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "logdir = './full_approach_training_history/logs/' + time_stamp()\n",
    "\n",
    "## Check whether the target directoy exist and \n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "    \n",
    "## Create callback for tensorboard/trainings-history with the path to the logs directory\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Model saving\n",
    "## Create directory which will contain trained models \n",
    "model_directory = \"./full_approach_training_history/saved_models/\" + time_stamp()\n",
    "\n",
    "## Check whether the target directoy exist and \n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "## Create callback which stores model-weights checkpoints\n",
    "checkpointer = ModelCheckpoint(filepath=model_directory + '/weights.{epoch:03d}-{val_loss:.2f}.hdf5', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.001)\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=10, baseline=0.8, restore_best_weights=True)\n",
    "csv_logger = CSVLogger(model_directory + '/training.log')\n",
    "\n",
    "# Generate callback list \n",
    "callbacks = [tensorboard, checkpointer, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (6) Train model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model on trainings data \n",
    "   \n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    verbose=1, \n",
    "                    shuffle=True, \n",
    "                    epochs=n_epochs, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=callbacks)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (7) Predict and evaluate <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot history \n",
    "def plot_loss_acc(model):\n",
    "  color_red = 'tab:red'\n",
    "  color_blue = 'tab:blue'\n",
    "\n",
    "  epochs=np.array(model.history.epoch)+1 # Add one to the list of epochs which is zero-indexed\n",
    "\n",
    "  loss=np.array(model.history.history['loss'])\n",
    "  acc=np.array(model.history.history['acc'])\n",
    "    \n",
    "  val_loss=np.array(model.history.history['val_loss'])\n",
    "  val_acc=np.array(model.history.history['val_acc'])\n",
    "  \n",
    "  # Create subplots\n",
    "  fig, (ax1, ax3) = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "  # TRAING DIAGRAM\n",
    "  ax1.title.set_text('TRAININGS DATA')\n",
    "  \n",
    "  ax1.set_xlabel('Epochs',fontsize=15)\n",
    "  ax1.set_ylabel('Loss', color=color_red,fontsize=15)\n",
    "  ax1.plot(epochs, loss, color=color_red,lw=2)\n",
    "  ax1.tick_params(axis='y', labelcolor=color_red)\n",
    "  ax1.grid(True)\n",
    "\n",
    "  ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "  ax2.set_ylabel('Accuracy', color=color_blue,fontsize=15)\n",
    "  ax2.plot(epochs, acc, color=color_blue,lw=2)\n",
    "  ax2.tick_params(axis='y', labelcolor=color_blue)\n",
    "\n",
    "  # VALIDATION DIAGRAM\n",
    "  ax3.title.set_text('VALIDATION DATA')\n",
    "  \n",
    "  ax3.set_xlabel('Epochs',fontsize=15)\n",
    "  ax3.set_ylabel('Validation Loss', color=color_red,fontsize=15)\n",
    "  ax3.plot(epochs, val_loss, dashes=[2, 2, 10, 2], color=color_red,lw=2)\n",
    "  ax3.tick_params(axis='y', labelcolor=color_red)\n",
    "  ax3.grid(True)\n",
    "\n",
    "  ax4 = ax3.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "  ax4.set_ylabel('Validation Accuracy', color=color_blue,fontsize=15)\n",
    "  ax4.plot(epochs, val_acc, dashes=[2, 2, 10, 2], color=color_blue,lw=2)\n",
    "  ax4.tick_params(axis='y', labelcolor=color_blue)\n",
    "  fig.tight_layout()\n",
    "\n",
    "  plt.legend(['Train', 'Test'], loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_loss_acc(model)\n",
    "\n",
    "# Plot evaluation\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Train accuracy: ' + str(train_acc))\n",
    "print('Test accuracy: ' + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (8) Save model <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "saved_model = model_directory + \"/model_\" + time_stamp() + \".h5\"\n",
    "model.save(saved_model)\n",
    "\n",
    "# Convert trained model into tflite model \n",
    "#converter = tf.lite.TFLiteConverter.from_saved_model('./trained_model.h5')\n",
    "#tflite_model = converter.convert()\n",
    "#open(model_directory + \"/lite_model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "\n",
    "# Convert trained model into CoreML\n",
    "coreml_model = coremltools.converters.keras.convert(saved_model, \n",
    "                                                    class_labels=LABELS, \n",
    "                                                    input_names=['pose'])\n",
    "\n",
    "#tf_converter.convert(tf_model_path=saved_model,\n",
    "#                     mlmodel_path='coreml_model.mlmodel',\n",
    "#                     output_feature_names=['softmax'],\n",
    "#                     input_name_shape_dict={'input': [1, 227, 227, 3]},\n",
    "#                     use_coreml_3=True)\n",
    "\n",
    "print(coreml_model)\n",
    "\n",
    "# Store CoreML model \n",
    "coreml_model.save(model_directory + \"/mlmodel_\" + time_stamp() + \".mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
